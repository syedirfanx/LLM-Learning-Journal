{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI4i--l48djD",
        "outputId": "1af1a8f9-c9f3-4a1c-9221-2a3c8c324844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "In this notebook, we will explore **two approaches to tokenization** for Large Language Models:\n",
        "\n",
        "1. **Manual Tokenization**  \n",
        "   - We will implement a custom tokenization process using **regular expressions (regex)**.  \n",
        "   - This approach will split text into tokens such that:  \n",
        "     - Each **word** and **special character** (`, . : ; ? _ ! \" ( ) ' --`) is treated as an individual token.  \n",
        "     - **Whitespace** will be used for splitting but will **not appear as tokens**.  \n",
        "   - This helps us understand the basic mechanics of tokenization.\n",
        "\n",
        "2. **Byte Pair Encoding (BPE) Tokenization**  \n",
        "   - We will use a library-based implementation of **BPE tokenization**, which is widely used in LLMs like GPT.  \n",
        "   - BPE combines frequent character pairs into tokens to reduce the overall vocabulary size while preserving common patterns.  \n",
        "   - This approach is **more efficient** and **closer to what real-world models use**.\n"
      ],
      "metadata": {
        "id": "KZm6S_iHXRm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Manual Tokenization**"
      ],
      "metadata": {
        "id": "d97e-vimViWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/LLM/Nebula by ChatGPT.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "print(len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM-HOoaF_YO3",
        "outputId": "788157ce-e942-47cf-c63d-5a670713c99c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8716\n",
            "Nebula: The Cosmic Cradles of Creation\n",
            "Preface\n",
            "When you look at the night sky, you see thousands of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the given text into tokens based on the following rules:\n",
        "\n",
        "- **Split words and the following special characters as individual tokens:**  \n",
        "  `, . : ; ? _ ! \" ( ) ' --`  \n",
        "- **Split on whitespace, but do not include whitespace as tokens**  \n",
        "- **Remove any empty strings after splitting**  "
      ],
      "metadata": {
        "id": "uOhDBGYbWLWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "id": "ZfL1D5NABX_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11eb4198-69a9-45f1-dbed-e5e936de1668"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nebula', ':', 'The', 'Cosmic', 'Cradles', 'of', 'Creation', 'Preface', 'When', 'you', 'look', 'at', 'the', 'night', 'sky', ',', 'you', 'see', 'thousands', 'of', 'stars', 'scattered', 'like', 'glitter', 'across', 'a', 'black', 'canvas', '.', 'It’s']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9wWe72howUj",
        "outputId": "4f53b20e-83d6-4684-d8dd-6d72440d36b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building the Vocabulary\n",
        "\n",
        "- After tokenizing the text, we need to create a **vocabulary** – a mapping of each unique token to a unique integer ID.\n",
        "- Steps:\n",
        "  1. **Get all unique tokens** from the tokenized text using `set(preprocessed)`.\n",
        "  2. **Sort the tokens** to maintain a consistent order.\n",
        "  3. **Add special tokens**:\n",
        "     - `<|endoftext|>` → Marks the end of the text sequence.\n",
        "     - `<|unk|>` → Represents unknown tokens (tokens not in the vocabulary).\n",
        "  4. **Create a dictionary** where:\n",
        "     - **Key = token**\n",
        "     - **Value = integer ID** (starting from 0)\n",
        "\n",
        "This vocabulary will be used for encoding text into numerical representations.\n"
      ],
      "metadata": {
        "id": "gg0smIpl26EJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU2X9itlo92e",
        "outputId": "a9050cdc-843c-4753-d0e5-27cede5f6c30"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 10:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv9Zd3DRpDqG",
        "outputId": "2c987f68-0fe8-45dc-85cc-11d4fa70c625"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('&', 1)\n",
            "('(', 2)\n",
            "(')', 3)\n",
            "(',', 4)\n",
            "('.', 5)\n",
            "('//hubblesite', 6)\n",
            "('//www', 7)\n",
            "('000', 8)\n",
            "('1', 9)\n",
            "('10', 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "xkaq3iJn2nag"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementing a Tokenizer\n",
        "\n",
        "We define a custom `Tokenizer` class to **encode text into token IDs** and **decode token IDs back to text**, based on our vocabulary.\n",
        "\n",
        "#### **Key Components**\n",
        "\n",
        "1. **Initialization (`__init__`)**\n",
        "   - `self.str_to_int`: A dictionary mapping tokens → integer IDs.\n",
        "   - `self.int_to_str`: A reverse dictionary mapping integer IDs → tokens.\n",
        "\n",
        "2. **`encode(text)`**\n",
        "   - Splits the input text using the same regex rules from earlier.\n",
        "   - Removes whitespace tokens and keeps special characters as tokens.\n",
        "   - Replaces any unknown token with `<|unk|>`.\n",
        "   - Converts tokens into their corresponding integer IDs using the vocabulary.\n",
        "   - **Returns**: A list of token IDs.\n",
        "\n",
        "3. **`decode(ids)`**\n",
        "   - Converts token IDs back into tokens using the reverse dictionary.\n",
        "   - Joins tokens with spaces.\n",
        "   - Uses a regex substitution to remove unwanted spaces before punctuation.\n",
        "   - **Returns**: The reconstructed text.\n",
        "\n",
        "#### **Purpose**\n",
        "This tokenizer demonstrates a **basic rule-based encoding and decoding process**, similar to how real tokenizers work, but on a smaller scale."
      ],
      "metadata": {
        "id": "H-PK-E9q3l1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "fxI2RZwoplBG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoding and Decoding\n",
        "\n",
        "We use the tokenizer to convert text into token IDs and then decode those IDs back into text.  \n",
        "- **Encoding**: Splits text into tokens and maps each token to its corresponding ID from the vocabulary. Unknown tokens are replaced with `<|unk|>`.  \n",
        "- **Decoding**: Converts token IDs back to tokens, joins them into a string, and removes extra spaces before punctuation.  \n",
        "\n",
        "This verifies that our tokenizer supports a complete **encode → decode cycle**, a crucial step for text processing in LLMs.\n"
      ],
      "metadata": {
        "id": "hv4Z44WE4Sgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(vocab)\n",
        "text = 'Nebulae are vast clouds of gas and dust in space, acting as the birthplaces and graveyards of stars, shaping the life cycle of the cosmos. They come in stunning forms—glowing, dark, or shattered remnants—each telling a story of creation, destruction, and renewal in the universe.'"
      ],
      "metadata": {
        "id": "jVUQ8D1rzLaA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "GsqBUUT52Bkb",
        "outputId": "4a02326c-70be-4012-c361-eec4dd005693"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Nebulae are vast clouds of gas and dust in space, acting as the birthplaces and graveyards of stars, shaping the life cycle of the cosmos. They come in stunning forms—glowing, dark, or shattered remnants—each telling a story of creation, destruction, and renewal in the universe.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B8n8eeH1nPj",
        "outputId": "999df66f-c77e-4e25-eaa6-ed46a50188e8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[126,\n",
              " 207,\n",
              " 632,\n",
              " 273,\n",
              " 486,\n",
              " 365,\n",
              " 203,\n",
              " 314,\n",
              " 400,\n",
              " 570,\n",
              " 4,\n",
              " 191,\n",
              " 211,\n",
              " 602,\n",
              " 664,\n",
              " 203,\n",
              " 664,\n",
              " 486,\n",
              " 580,\n",
              " 4,\n",
              " 664,\n",
              " 602,\n",
              " 428,\n",
              " 664,\n",
              " 486,\n",
              " 602,\n",
              " 664,\n",
              " 5,\n",
              " 165,\n",
              " 283,\n",
              " 400,\n",
              " 588,\n",
              " 664,\n",
              " 4,\n",
              " 299,\n",
              " 4,\n",
              " 493,\n",
              " 664,\n",
              " 664,\n",
              " 664,\n",
              " 187,\n",
              " 584,\n",
              " 486,\n",
              " 296,\n",
              " 4,\n",
              " 307,\n",
              " 4,\n",
              " 203,\n",
              " 664,\n",
              " 400,\n",
              " 602,\n",
              " 628,\n",
              " 5]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "eeb_mIeR2C8O",
        "outputId": "4594d74f-3dd2-480c-9af7-9073a1866437"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Nebulae are vast clouds of gas and dust in space, acting as the <|unk|> and <|unk|> of stars, <|unk|> the life <|unk|> of the <|unk|>. They come in stunning <|unk|>, dark, or <|unk|> <|unk|> <|unk|> a story of creation, destruction, and <|unk|> in the universe.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Byte Pair Tokenizer**"
      ],
      "metadata": {
        "id": "72d9xz3YgpIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install tiktoken"
      ],
      "metadata": {
        "id": "YkELgppigsuz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "CFPjrNQgg0Zu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BPE Tokenization using `tiktoken` (GPT-2 Encoding)\n",
        "\n",
        "In this step, we use the **`tiktoken`** library to apply **Byte Pair Encoding (BPE)** tokenization, the same method used in GPT models.\n",
        "\n",
        "##### **What happens**\n",
        "- **Tokenizer Initialization**:  \n",
        "  `tiktoken.get_encoding(\"gpt2\")` loads the GPT-2 BPE tokenizer.\n",
        "  \n",
        "- **Encoding**:  \n",
        "  `tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})` converts the text into a list of integer token IDs.  \n",
        "  - Uses GPT-2’s BPE vocabulary.\n",
        "  - Allows the special token `<|endoftext|>` to remain intact.\n",
        "\n",
        "- **Decoding**:  \n",
        "  `tokenizer.decode(integers)` converts the token IDs back into the original text.\n",
        "\n",
        "##### **Purpose**\n",
        "This demonstrates **library-based BPE tokenization**, which is more efficient and commonly used in LLMs compared to simple rule-based tokenization.\n"
      ],
      "metadata": {
        "id": "BUnuPnne6kci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "EwFSReYMg_kj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    'Nebulae are enormous clouds of gas and dust scattered across the universe, serving as the cradles where stars are born. These majestic formations can span hundreds of light-years and display breathtaking colors when illuminated by young, hot stars. <|endoftext|> Some nebulae glow brightly as emission regions, while others appear dark, blocking the starlight behind them. They play a crucial role in recycling elements, giving rise to new stars and planetary systems over billions of years. Studying nebulae helps scientists understand the origin of stars, planets, and even life itself.'\n",
        ")"
      ],
      "metadata": {
        "id": "6sJPVE8TUiB4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHggo4lDVHFf",
        "outputId": "448bb888-c213-40cf-d991-104e7c78a88c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[45, 1765, 377, 3609, 389, 9812, 15114, 286, 3623, 290, 8977, 16830, 1973, 262, 6881, 11, 7351, 355, 262, 1067, 324, 829, 810, 5788, 389, 4642, 13, 2312, 45308, 30648, 460, 11506, 5179, 286, 1657, 12, 19002, 290, 3359, 35589, 7577, 618, 35162, 416, 1862, 11, 3024, 5788, 13, 220, 50256, 2773, 497, 15065, 3609, 19634, 35254, 355, 25592, 7652, 11, 981, 1854, 1656, 3223, 11, 12013, 262, 3491, 2971, 2157, 606, 13, 1119, 711, 257, 8780, 2597, 287, 25914, 4847, 11, 3501, 4485, 284, 649, 5788, 290, 27047, 3341, 625, 13188, 286, 812, 13, 3604, 1112, 497, 15065, 3609, 5419, 5519, 1833, 262, 8159, 286, 5788, 11, 14705, 11, 290, 772, 1204, 2346, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "BjhGxZADVL6d",
        "outputId": "51bd3180-5ed1-4d5a-de55-02a4be832533"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Nebulae are enormous clouds of gas and dust scattered across the universe, serving as the cradles where stars are born. These majestic formations can span hundreds of light-years and display breathtaking colors when illuminated by young, hot stars. <|endoftext|> Some nebulae glow brightly as emission regions, while others appear dark, blocking the starlight behind them. They play a crucial role in recycling elements, giving rise to new stars and planetary systems over billions of years. Studying nebulae helps scientists understand the origin of stars, planets, and even life itself.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}