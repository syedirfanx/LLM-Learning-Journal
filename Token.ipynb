{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI4i--l48djD",
        "outputId": "48814453-a345-4444-f202-af1d2321f1df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "In this notebook, we will explore **two approaches to tokenization** for Large Language Models:\n",
        "\n",
        "1. **Manual Tokenization**  \n",
        "   - We will implement a custom tokenization process using **regular expressions (regex)**.  \n",
        "   - This approach will split text into tokens such that:  \n",
        "     - Each **word** and **special character** (`, . : ; ? _ ! \" ( ) ' --`) is treated as an individual token.  \n",
        "     - **Whitespace** will be used for splitting but will **not appear as tokens**.  \n",
        "   - This helps us understand the basic mechanics of tokenization.\n",
        "\n",
        "2. **Byte Pair Encoding (BPE) Tokenization**  \n",
        "   - We will use a library-based implementation of **BPE tokenization**, which is widely used in LLMs like GPT.  \n",
        "   - BPE combines frequent character pairs into tokens to reduce the overall vocabulary size while preserving common patterns.  \n",
        "   - This approach is **more efficient** and **closer to what real-world models use**.\n"
      ],
      "metadata": {
        "id": "KZm6S_iHXRm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Manual Tokenization**"
      ],
      "metadata": {
        "id": "d97e-vimViWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/LLM/Nebula by ChatGPT.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "print(len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM-HOoaF_YO3",
        "outputId": "f9319aaf-10ab-4094-b014-7235dc935691"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8716\n",
            "Nebula: The Cosmic Cradles of Creation\n",
            "Preface\n",
            "When you look at the night sky, you see thousands of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Rule-based Tokenization\n",
        "Split the given text into tokens based on the following rules:\n",
        "\n",
        "- **Split words and the following special characters as individual tokens:**  \n",
        "  `, . : ; ? _ ! \" ( ) ' --`  \n",
        "- **Split on whitespace, but do not include whitespace as tokens**  \n",
        "- **Remove any empty strings after splitting**  "
      ],
      "metadata": {
        "id": "uOhDBGYbWLWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "id": "ZfL1D5NABX_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da5ee2c-5f4d-4b8c-b684-ea6daae39f79"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nebula', ':', 'The', 'Cosmic', 'Cradles', 'of', 'Creation', 'Preface', 'When', 'you', 'look', 'at', 'the', 'night', 'sky', ',', 'you', 'see', 'thousands', 'of', 'stars', 'scattered', 'like', 'glitter', 'across', 'a', 'black', 'canvas', '.', 'It’s']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9wWe72howUj",
        "outputId": "00e5cdaf-dd95-4558-ca9c-f13ad09103e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Building the Vocabulary\n",
        "\n",
        "- After tokenizing the text, we need to create a **vocabulary** – a mapping of each unique token to a unique integer ID.\n",
        "- Steps:\n",
        "  1. **Get all unique tokens** from the tokenized text using `set(preprocessed)`.\n",
        "  2. **Sort the tokens** to maintain a consistent order.\n",
        "  3. **Add special tokens**:\n",
        "     - `<|endoftext|>` → Marks the end of the text sequence.\n",
        "     - `<|unk|>` → Represents unknown tokens (tokens not in the vocabulary).\n",
        "  4. **Create a dictionary** where:\n",
        "     - **Key = token**\n",
        "     - **Value = integer ID** (starting from 0)\n",
        "\n",
        "This vocabulary will be used for encoding text into numerical representations.\n"
      ],
      "metadata": {
        "id": "gg0smIpl26EJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU2X9itlo92e",
        "outputId": "469beea8-686f-4485-fdb6-3f71d56372eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 10:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv9Zd3DRpDqG",
        "outputId": "583f4f4e-978c-4f13-9748-d2a39376d573"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('&', 1)\n",
            "('(', 2)\n",
            "(')', 3)\n",
            "(',', 4)\n",
            "('.', 5)\n",
            "('//hubblesite', 6)\n",
            "('//www', 7)\n",
            "('000', 8)\n",
            "('1', 9)\n",
            "('10', 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "xkaq3iJn2nag"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Implementing a Tokenizer\n",
        "\n",
        "We define a custom `Tokenizer` class to **encode text into token IDs** and **decode token IDs back to text**, based on our vocabulary.\n",
        "\n",
        "#### **Key Components**\n",
        "\n",
        "1. **Initialization (`__init__`)**\n",
        "   - `self.str_to_int`: A dictionary mapping tokens → integer IDs.\n",
        "   - `self.int_to_str`: A reverse dictionary mapping integer IDs → tokens.\n",
        "\n",
        "2. **`encode(text)`**\n",
        "   - Splits the input text using the same regex rules from earlier.\n",
        "   - Removes whitespace tokens and keeps special characters as tokens.\n",
        "   - Replaces any unknown token with `<|unk|>`.\n",
        "   - Converts tokens into their corresponding integer IDs using the vocabulary.\n",
        "   - **Returns**: A list of token IDs.\n",
        "\n",
        "3. **`decode(ids)`**\n",
        "   - Converts token IDs back into tokens using the reverse dictionary.\n",
        "   - Joins tokens with spaces.\n",
        "   - Uses a regex substitution to remove unwanted spaces before punctuation.\n",
        "   - **Returns**: The reconstructed text.\n",
        "\n",
        "#### **Purpose**\n",
        "This tokenizer demonstrates a **basic rule-based encoding and decoding process**, similar to how real tokenizers work, but on a smaller scale."
      ],
      "metadata": {
        "id": "H-PK-E9q3l1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "fxI2RZwoplBG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4 Encoding and Decoding\n",
        "\n",
        "We use the tokenizer to convert text into token IDs and then decode those IDs back into text.  \n",
        "- **Encoding**: Splits text into tokens and maps each token to its corresponding ID from the vocabulary. Unknown tokens are replaced with `<|unk|>`.  \n",
        "- **Decoding**: Converts token IDs back to tokens, joins them into a string, and removes extra spaces before punctuation.  \n",
        "\n",
        "This verifies that our tokenizer supports a complete **encode → decode cycle**, a crucial step for text processing in LLMs.\n"
      ],
      "metadata": {
        "id": "hv4Z44WE4Sgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(vocab)\n",
        "text = 'Nebulae are vast clouds of gas and dust in space, acting as the birthplaces and graveyards of stars, shaping the life cycle of the cosmos. They come in stunning forms—glowing, dark, or shattered remnants—each telling a story of creation, destruction, and renewal in the universe.'"
      ],
      "metadata": {
        "id": "jVUQ8D1rzLaA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "GsqBUUT52Bkb",
        "outputId": "453455f0-fd66-4e4e-8d76-40eb90aece0b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Nebulae are vast clouds of gas and dust in space, acting as the birthplaces and graveyards of stars, shaping the life cycle of the cosmos. They come in stunning forms—glowing, dark, or shattered remnants—each telling a story of creation, destruction, and renewal in the universe.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B8n8eeH1nPj",
        "outputId": "565acdac-1821-4fba-fa18-b5b1e20fa2c5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[126,\n",
              " 207,\n",
              " 632,\n",
              " 273,\n",
              " 486,\n",
              " 365,\n",
              " 203,\n",
              " 314,\n",
              " 400,\n",
              " 570,\n",
              " 4,\n",
              " 191,\n",
              " 211,\n",
              " 602,\n",
              " 664,\n",
              " 203,\n",
              " 664,\n",
              " 486,\n",
              " 580,\n",
              " 4,\n",
              " 664,\n",
              " 602,\n",
              " 428,\n",
              " 664,\n",
              " 486,\n",
              " 602,\n",
              " 664,\n",
              " 5,\n",
              " 165,\n",
              " 283,\n",
              " 400,\n",
              " 588,\n",
              " 664,\n",
              " 4,\n",
              " 299,\n",
              " 4,\n",
              " 493,\n",
              " 664,\n",
              " 664,\n",
              " 664,\n",
              " 187,\n",
              " 584,\n",
              " 486,\n",
              " 296,\n",
              " 4,\n",
              " 307,\n",
              " 4,\n",
              " 203,\n",
              " 664,\n",
              " 400,\n",
              " 602,\n",
              " 628,\n",
              " 5]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "eeb_mIeR2C8O",
        "outputId": "8f6b38dd-1b41-401b-ffea-63b2689e34ee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Nebulae are vast clouds of gas and dust in space, acting as the <|unk|> and <|unk|> of stars, <|unk|> the life <|unk|> of the <|unk|>. They come in stunning <|unk|>, dark, or <|unk|> <|unk|> <|unk|> a story of creation, destruction, and <|unk|> in the universe.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Byte Pair Tokenizer**"
      ],
      "metadata": {
        "id": "72d9xz3YgpIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install tiktoken"
      ],
      "metadata": {
        "id": "YkELgppigsuz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "CFPjrNQgg0Zu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BPE Tokenization using `tiktoken` (GPT-2 Encoding)\n",
        "\n",
        "In this step, we use the **`tiktoken`** library to apply **Byte Pair Encoding (BPE)** tokenization, the same method used in GPT models.\n",
        "\n",
        "##### **What happens**\n",
        "- **Tokenizer Initialization**:  \n",
        "  `tiktoken.get_encoding(\"gpt2\")` loads the GPT-2 BPE tokenizer.\n",
        "  \n",
        "- **Encoding**:  \n",
        "  `tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})` converts the text into a list of integer token IDs.  \n",
        "  - Uses GPT-2’s BPE vocabulary.\n",
        "  - Allows the special token `<|endoftext|>` to remain intact.\n",
        "\n",
        "- **Decoding**:  \n",
        "  `tokenizer.decode(integers)` converts the token IDs back into the original text.\n",
        "\n",
        "##### **Purpose**\n",
        "This demonstrates **library-based BPE tokenization**, which is more efficient and commonly used in LLMs compared to simple rule-based tokenization.\n"
      ],
      "metadata": {
        "id": "BUnuPnne6kci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "EwFSReYMg_kj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    'Nebulae are enormous clouds of gas and dust scattered across the universe, serving as the cradles where stars are born. These majestic formations can span hundreds of light-years and display breathtaking colors when illuminated by young, hot stars. <|endoftext|> Some nebulae glow brightly as emission regions, while others appear dark, blocking the starlight behind them. They play a crucial role in recycling elements, giving rise to new stars and planetary systems over billions of years. Studying nebulae helps scientists understand the origin of stars, planets, and even life itself.'\n",
        ")"
      ],
      "metadata": {
        "id": "6sJPVE8TUiB4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHggo4lDVHFf",
        "outputId": "c5b51f76-7d8e-442e-8cb5-0cc7e0c56e9f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[45, 1765, 377, 3609, 389, 9812, 15114, 286, 3623, 290, 8977, 16830, 1973, 262, 6881, 11, 7351, 355, 262, 1067, 324, 829, 810, 5788, 389, 4642, 13, 2312, 45308, 30648, 460, 11506, 5179, 286, 1657, 12, 19002, 290, 3359, 35589, 7577, 618, 35162, 416, 1862, 11, 3024, 5788, 13, 220, 50256, 2773, 497, 15065, 3609, 19634, 35254, 355, 25592, 7652, 11, 981, 1854, 1656, 3223, 11, 12013, 262, 3491, 2971, 2157, 606, 13, 1119, 711, 257, 8780, 2597, 287, 25914, 4847, 11, 3501, 4485, 284, 649, 5788, 290, 27047, 3341, 625, 13188, 286, 812, 13, 3604, 1112, 497, 15065, 3609, 5419, 5519, 1833, 262, 8159, 286, 5788, 11, 14705, 11, 290, 772, 1204, 2346, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "BjhGxZADVL6d",
        "outputId": "beb85297-8a5a-4082-b930-cbf93f7794af"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Nebulae are enormous clouds of gas and dust scattered across the universe, serving as the cradles where stars are born. These majestic formations can span hundreds of light-years and display breathtaking colors when illuminated by young, hot stars. <|endoftext|> Some nebulae glow brightly as emission regions, while others appear dark, blocking the starlight behind them. They play a crucial role in recycling elements, giving rise to new stars and planetary systems over billions of years. Studying nebulae helps scientists understand the origin of stars, planets, and even life itself.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Input-Target Pairs**"
      ],
      "metadata": {
        "id": "DKg_-mdA4EaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO5YPDlb4JCv",
        "outputId": "01cf4572-b975-4d53-c155-ebde808fde39"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We've got 2066 tokens after applying BPE tokenizer*"
      ],
      "metadata": {
        "id": "_Sp2cxUd51KX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 Creating Input and Target Sequences\n",
        "\n",
        "We define a fixed context size of 8 tokens to prepare sequences for training. The input sequence (`x`) consists of the first 8 tokens from the encoded text, while the target sequence (`y`) is a shifted version of the text, starting 4 tokens later.  \n",
        "\n",
        "This method creates overlapping input-target pairs, which helps the model learn next-token prediction by using part of the context and predicting upcoming tokens.\n"
      ],
      "metadata": {
        "id": "_18WQAcqWMtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 8\n",
        "\n",
        "x = enc_text[:context_size]\n",
        "y = enc_text[4:context_size+4]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:                     {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYWAbK995c7k",
        "outputId": "5489c93b-e20a-45cc-c4cb-c4300b76ef2c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [45, 1765, 4712, 25, 383, 32011, 3864, 324]\n",
            "y:                     [383, 32011, 3864, 324, 829, 286, 21582, 198]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 Generating Context-Target Pairs\n",
        "\n",
        "We iterate through the encoded text to generate training samples for next-token prediction. For each step:\n",
        "\n",
        "- **Context**: A sequence of tokens from the start up to the current position.\n",
        "- **Desired Token**: The next token that should be predicted given the context.\n",
        "\n",
        "This approach demonstrates how language models learn by predicting the next token based on the preceding tokens.\n"
      ],
      "metadata": {
        "id": "Ed-g5A25XJaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4, context_size+4):\n",
        "    context = enc_text[:i]\n",
        "    desired = enc_text[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1jHuuKb7h8F",
        "outputId": "aafc1391-9695-468d-ff08-e68372157ea8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[45, 1765, 4712, 25] ----> 383\n",
            "[45, 1765, 4712, 25, 383] ----> 32011\n",
            "[45, 1765, 4712, 25, 383, 32011] ----> 3864\n",
            "[45, 1765, 4712, 25, 383, 32011, 3864] ----> 324\n",
            "[45, 1765, 4712, 25, 383, 32011, 3864, 324] ----> 829\n",
            "[45, 1765, 4712, 25, 383, 32011, 3864, 324, 829] ----> 286\n",
            "[45, 1765, 4712, 25, 383, 32011, 3864, 324, 829, 286] ----> 21582\n",
            "[45, 1765, 4712, 25, 383, 32011, 3864, 324, 829, 286, 21582] ----> 198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3 Displaying Context-Target Pairs in Text Form\n",
        "\n",
        "We decode each context and target token back into readable text. This makes it easier to understand how the model sees input-output pairs:\n",
        "\n",
        "This visualization helps us clearly see how language models are trained to predict the next word based on the preceding context.\n"
      ],
      "metadata": {
        "id": "2qt2DDzsYuXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4, context_size+4):\n",
        "    context = enc_text[:i]\n",
        "    desired = enc_text[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bx_K6zM8biy",
        "outputId": "3b2f0fbd-9efb-484f-fc30-90d621c33e18"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nebula: ---->  The\n",
            "Nebula: The ---->  Cosmic\n",
            "Nebula: The Cosmic ---->  Cr\n",
            "Nebula: The Cosmic Cr ----> ad\n",
            "Nebula: The Cosmic Crad ----> les\n",
            "Nebula: The Cosmic Cradles ---->  of\n",
            "Nebula: The Cosmic Cradles of ---->  Creation\n",
            "Nebula: The Cosmic Cradles of Creation ----> \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.4 Creating a Dataset and DataLoader for GPT Training\n",
        "\n",
        "In this step, we prepare the text data for training by converting it into overlapping input-target sequences using **PyTorch's Dataset and DataLoader classes**.\n",
        "\n",
        "##### **Key Components**\n",
        "\n",
        "1. **GPTDataset Class**\n",
        "   - **Initialization (`__init__`)**:\n",
        "     - Encodes the raw text into token IDs using the GPT-2 BPE tokenizer.\n",
        "     - Splits token IDs into overlapping **input chunks** and **target chunks**:\n",
        "       - `input_chunk`: A sequence of `max_length` tokens.\n",
        "       - `target_chunk`: The next `max_length` tokens shifted by one position (for next-token prediction).\n",
        "     - Stores all input-target pairs as tensors.\n",
        "   - **`__len__`**: Returns the total number of input-target pairs.\n",
        "   - **`__getitem__`**: Retrieves a single input-target pair by index.\n",
        "\n",
        "2. **`create_dataloader` Function**\n",
        "   - Wraps the dataset into a **DataLoader** for efficient batching.\n",
        "   - Parameters:\n",
        "     - `batch_size`: Number of sequences per batch.\n",
        "     - `max_length`: Context size for each input sequence.\n",
        "     - `stride`: Step size for creating overlapping chunks.\n",
        "\n",
        "3. **Example**\n",
        "   - Creates a DataLoader for the raw text with:\n",
        "     - `batch_size = 8`\n",
        "     - `max_length = 4`\n",
        "     - `stride = 4`\n",
        "   - Fetches the first batch and prints **inputs** and **targets**.\n",
        "\n",
        "##### **Purpose**\n",
        "This process converts a raw text into a structured dataset of overlapping input-target pairs, enabling **efficient batch training for next-token prediction models like GPT**.\n"
      ],
      "metadata": {
        "id": "175WRbrVZkKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "dj3FbjT7R0O0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "LtMcn0pqUOCI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULsZemZuUTN2",
        "outputId": "bcf45087-e0dd-45bd-d395-10310c6c12e7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   45,  1765,  4712,    25],\n",
            "        [  383, 32011,  3864,   324],\n",
            "        [  829,   286, 21582,   198],\n",
            "        [ 6719,  2550,   198,  2215],\n",
            "        [  345,   804,   379,   262],\n",
            "        [ 1755,  6766,    11,   345],\n",
            "        [  766,  4138,   286,  5788],\n",
            "        [16830,   588, 31133,  1973]])\n",
            "\n",
            "Targets:\n",
            " tensor([[ 1765,  4712,    25,   383],\n",
            "        [32011,  3864,   324,   829],\n",
            "        [  286, 21582,   198,  6719],\n",
            "        [ 2550,   198,  2215,   345],\n",
            "        [  804,   379,   262,  1755],\n",
            "        [ 6766,    11,   345,   766],\n",
            "        [ 4138,   286,  5788, 16830],\n",
            "        [  588, 31133,  1973,   257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Input Embeddings**"
      ],
      "metadata": {
        "id": "2cFZRkPpe2kh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1 Word Embeddings\n",
        "\n",
        "This example demonstrates how to use **`torch.nn.Embedding`** to map discrete token indices into dense vector representations.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ve0vXXMGkbwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ],
      "metadata": {
        "id": "HpX3Q3zCe2Kw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "LCyWGhaEjPnu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2HwmX6pjTR6",
        "outputId": "03ab81bf-ccf6-411f-ff2f-e0e7a56bd60d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2YtKl26jopa",
        "outputId": "6489564c-8e02-48c6-9054-aba1de89225f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2 Positional Embeddings"
      ],
      "metadata": {
        "id": "PygkGlqilRm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "2AZDg4Tsj8tt"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ],
      "metadata": {
        "id": "KynP9A5vlrKj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yMfKM8nlt-E",
        "outputId": "aea10eb5-6eca-48f6-ca88-dd3dd4d0bf3d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   45,  1765,  4712,    25],\n",
            "        [  383, 32011,  3864,   324],\n",
            "        [  829,   286, 21582,   198],\n",
            "        [ 6719,  2550,   198,  2215],\n",
            "        [  345,   804,   379,   262],\n",
            "        [ 1755,  6766,    11,   345],\n",
            "        [  766,  4138,   286,  5788],\n",
            "        [16830,   588, 31133,  1973]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AxsSNRil0zk",
        "outputId": "7a03f4fa-6b86-4d8f-b9fd-524d5f65b883"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ],
      "metadata": {
        "id": "KiO_j2M5l4vZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vuq2Rcc5l9Zc",
        "outputId": "f5c04610-8e27-4b98-a730-69f5197fb83c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9di_oCu7l_Dc",
        "outputId": "c907d6f8-9dc0-4487-dcc3-e544fc62f245"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    }
  ]
}